---
layout: post
title: Shopping in AR with the help of AI
subtitle: Soph-AR
video: https://cdn.shopify.com/videos/c/o/v/c041e6bfa71e4dcaa42d80bdb08a0872.mp4
---

Sometimes an idea comes along that feels so natural and obvious that it would be wrong to _not_ do it.

The other week our colleagues in Shop launched their OpenAI collaboration, tying the power of ChatGPT to the extensive Shopify product library and backend.

<div class="tweet-wrapper">
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Shop smarter with <a href="https://t.co/7YAW0Tk8Rh">https://t.co/7YAW0Tk8Rh</a>! We&#39;ve brought our ChatGPT-powered shopping assistant to the web ‚Äî try it out, and don‚Äôt forget to sign in with Shop to save your faves üíú <a href="https://t.co/DpSdLEr4QD">pic.twitter.com/DpSdLEr4QD</a></p>&mdash; Shop (@shop) <a href="https://twitter.com/shop/status/1636022946127831040?ref_src=twsrc%5Etfw">March 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>

On the Shop app or in the [browser](https://shop.app/ai/), you can shop and explore and ask questions just like you would in a real store with a knowledgeable human salesperson. ChatGPT serves up comments and accompanying photos, dialing in your budget and preferences and surfacing products that fit.

It‚Äôs so cool, and a huge leap from trying to finagle often vague filters and categories on websites to drill down into your preferences. Far more flexible, infinitely more personal ‚Äì we can wax poetic for days.

And for us, as a team that's focused on combining current and emerging technologies together, we could easily see a technology stack where it‚Äôs basically all of the same components except instead of displaying photos, it displays 3D models (which Shopify already has a massive library of), and instead of using a screen and keyboard, you use AR glasses and your voice.

Here‚Äôs our concept in a nutshell:

{% include shopify-video.html id='https://cdn.shopify.com/videos/c/o/v/bf28e23988f64b98b077f6d8ee9c7d22.mp4' width='100%' %}

But of course, we made this up in Blender (using our recently launched open source tool [Tracky]({{ site.baseurl }}{% link _posts/2023-03-09-Tracky.md %})), pre-recording the video and doing the animation, audio and speech synthesis in post.

So how much of it is real? How much can we do today?

It turns out quite a bit, with some caveats. The biggest one is: it‚Äôs slow.

[Video of prototype here]

There‚Äôs a tradeoff here. We‚Äôre using ChatGPT and you can choose between GPT-3.5-Turbo, which is an older and less accurate model but faster, or GPT-4 which is much higher quality in results, but has a pretty noticeable dead spot of lag in the conversation.

Eventually, of course, as these models get better and more efficient, we‚Äôll simply get high quality AND fast, but as of the time of this writing, we‚Äôve got what we‚Äôve got.

We‚Äôre using the Meta Quest Pro headset for this demo, which has colour passthrough, but isn‚Äôt quite as clean as our imagined fictional future-glasses. We extrapolate the curve of progress here too.

Nothing magic, just‚Ä¶ in the future still.

But otherwise this demo is pretty real overall.

- We can take your voice and convert it to text.
- We can send that text to ChatGPT as conversational input.
- We can receive ChatGPT‚Äôs answers and synthesize them through a voice model.
- We can load 3D models of products from Shopify's library and display them.
- We can pull features and prices and product information and display it.

These core components make up most of the [shop.ai](https://shop.app/ai/) loop: you‚Äôre asking questions, getting information, asking more questions, getting more suggestions and eventually reaching a winner of a product decision.

# Technical details

One notable point about this demo is that it‚Äôs not a native app, it‚Äôs a web app that knows how to present itself in WebXR. This brings enormous benefits - the ability for people to interact without first downloading a client, the ability for the content to adapt to future headsets and even extend to those without headsets, and the freedom to iterate and ship at your own pace.

Another neat aspect of this demo came about from a technical constraint: the Quest browser doesn‚Äôt support the [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API). This is understandable because it can take a lot of resources to detect, parse, and synthesize speech, and mobile XR headsets are always resource constrained.

To work around that we built a solution that opens a standard-fare WebRTC audio connection, and Soph ‚Äújoins your call.‚Äù Soph then listens for your speech, decides when to parse it, formulates a helpful response, and then generates a waveform response that is sent right back over to the audio call. This constraint-driven design means that our Soph implementation is extremely flexible. That is, any browser that can open an audio call could now connect to and talk with Soph!

WebRTC isn‚Äôt interesting, it‚Äôs old boring web conferencing tech, but the fact that it‚Äôs old boring web conferencing tech is emblematic of what‚Äôs so exciting right now. By using standard web technologies, we show that we can deliver the natural language experiences of the future to the devices of today, and they‚Äôre only going to get better from here on out!
