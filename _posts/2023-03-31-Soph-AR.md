---
layout: post
title: What does AR shopping look like in a world with AI?
subtitle: Soph-AR
video: https://cdn.shopify.com/videos/c/o/v/4885b0679156409198638641478dea2b.mp4
---

Sometimes an idea comes along that feels so natural and obvious that it would be wrong to _not_ do it.

The other week our colleagues in Shop launched their OpenAI collaboration, tying the power of ChatGPT to the extensive Shopify product library and backend.

<div class="tweet-wrapper">
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Shop smarter with <a href="https://t.co/7YAW0Tk8Rh">https://t.co/7YAW0Tk8Rh</a>! We&#39;ve brought our ChatGPT-powered shopping assistant to the web ‚Äî try it out, and don‚Äôt forget to sign in with Shop to save your faves üíú <a href="https://t.co/DpSdLEr4QD">pic.twitter.com/DpSdLEr4QD</a></p>&mdash; Shop (@shop) <a href="https://twitter.com/shop/status/1636022946127831040?ref_src=twsrc%5Etfw">March 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div>

On the Shop app or in the [browser](https://shop.app/ai/), you can shop and explore and ask questions just like you would in a real store with a knowledgeable human salesperson. ChatGPT serves up comments and accompanying photos, dialing in your budget and preferences and surfacing products that fit.

It‚Äôs so cool, and a huge leap from trying to finagle often vague filters and categories on websites to drill down into your preferences. Far more flexible, infinitely more personal ‚Äì we can wax poetic for days.

And for us, as a team that's focused on combining current and emerging technologies together, we could easily see a technology stack where it‚Äôs basically all of the same components except instead of displaying photos, it displays 3D models (which Shopify already has a massive library of), and instead of using a screen and keyboard, it uses AR glasses and your voice.

# Concept and prototype

Here‚Äôs our concept in a nutshell:

{% include shopify-video.html id='https://cdn.shopify.com/videos/c/o/v/bf28e23988f64b98b077f6d8ee9c7d22.mp4' width='100%' %}

But of course, we made this up in Blender (using our recently launched open source tool [Tracky]({{ site.baseurl }}{% link _posts/2023-03-09-Tracky.md %})), pre-recording the video and doing the animation, audio and speech synthesis in post.

So how much of it is real? How much can we do today?

It turns out quite a bit, with some caveats. The biggest one is: it‚Äôs slow.

[Video of prototype here]

There‚Äôs a tradeoff here. We‚Äôre using ChatGPT and you can choose between GPT-3.5-Turbo, which is an older and less accurate model but faster, or GPT-4 which is much higher quality in results, but has a pretty noticeable dead spot of lag in the conversation.

Eventually, of course, as these models get better and more efficient, we‚Äôll simply get high quality AND fast, but as of the time of this writing, we‚Äôve got what we‚Äôve got.

In the same vein, the text to speech synthesis has an identical tradeoff slider: quality VS. speed.

So the stack itself is fairly real, we‚Äôre just making a lot of quality concessions for time and speed:

- We can take your voice and convert it to text.
- We can send that text to ChatGPT as conversational input.
- We can receive ChatGPT‚Äôs answers and synthesize them through a voice model.
- We can load 3D models of products from Shopify's library and display them.
- We can pull features and prices and product information and display it.

These core components make up most of the [shop.ai](https://shop.app/ai/) loop: you‚Äôre asking questions, getting information, asking more questions, getting more suggestions and eventually reaching a winner of a product decision.

# The uncanny valley of speech synthesis

There are commercial text-to-speech models that do offline rendering (which is to say: you submit text and it takes some time to process and spit out a cleaner, better result) but with the real-time nature of our use here, we‚Äôre using models that are faster and weaker in order to generate speech on the fly.

The other thing is, open-source speech models right now aren‚Äôt as good as some of the commercial options. The recent breakthroughs in large-language models like GPT have been happening in the world of speech synthesis as well with breakthroughs like [VALL-E](https://valle-demo.github.io/) - where larger, simpler models are fed internet-scale datasets and vastly outperform their predecessors. 

Training these next-gen models, however, requires access to internet-scale datasets and the enormous compute resources needed to do these training runs. Additionally, these new models are so powerful and can mimic real voices so well, that corporations are credibly worried about releasing them to the public as open source. Which means that, although we expect this to change, open model weights dramatically trail the quality of commercial offerings at this time.

Here are a few clips comparing a few different models we played with:

CLIP 1

CLIP 2

The former is an off-the-shelf speech model called [Fast Pitch](https://arxiv.org/abs/2006.06873), trained on the [LJ Speech dataset](https://keithito.com/LJ-Speech-Dataset/), paired with a [full-band MelGAN vocoder](https://arxiv.org/abs/2005.05106) trained on the [LibriTTS dataset](https://research.google/resources/datasets/libri-tts/), all provided by the [CoquiTTS](https://github.com/coqui-ai/TTS) library. This solution is fast: it produces full responses in under a half second on an NVIDIA RTX 3080.

The latter is more interesting: we start with an off-the-shelf speech model from Microsoft called [SpeechT5](https://arxiv.org/abs/2110.07205), but this particular model allows you to encode the speaker‚Äôs tone, tenor, speaking style, etc. into a numeric value. We used about 30 seconds of voice recording from a colleague (actually, the exact lines recorded for the concept video) to calculate a numeric value representing their speech, and now we can use that value to condition all of the speech that the model generates. Pretty cool! But as you can hear, the quality of the synthesized speech needs improvement, and it‚Äôs not as quick-to-respond as the other model.

# On the power of WebXR and WebRTC

One notable point about this demo is that it‚Äôs not a native app, it‚Äôs a web app that knows how to present itself in WebXR. This brings enormous benefits - the ability for people to interact without first downloading a client, the ability for the content to adapt to future headsets and even extend to those without headsets, and the freedom to iterate and ship at your own pace.

Another neat aspect of this demo came about from a technical constraint: the Quest browser doesn‚Äôt support the [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API). This is understandable because it can take a lot of resources to detect, parse, and synthesize speech, and mobile XR headsets are always resource constrained.

To work around that we built a solution that opens a standard-fare WebRTC audio connection, and Soph ‚Äújoins your call.‚Äù Soph then listens for your speech, decides when to parse it, formulates a helpful response, and then generates a waveform response that is sent right back over to the audio call. This constraint-driven design means that our Soph implementation is extremely flexible. That is, any browser that can open an audio call could now connect to and talk with Soph!

WebRTC itself isn‚Äôt that interesting ‚Äì it‚Äôs old boring web conferencing tech ‚Äì but the fact that it‚Äôs old boring web conferencing tech is emblematic of what‚Äôs so exciting right now. By using standard web technologies, we show that we can deliver the natural language experiences of the future to the devices of today, and they‚Äôre only going to get better from here on out!

# Conclusions

An end-to-end prototype like this tells us two things. First, all the building blocks are there, and it‚Äôs possible to plug them together today. That‚Äôs important! It means we‚Äôre on the right track. But the second thing it tells us is where the quality bar is at. What is that intangible point at which you go from ‚Äúcool toy‚Äù to ‚Äúirreplaceable tool‚Äù? We know we‚Äôre not there yet, but we know where we‚Äôre going and how to get there. And at the current pace of innovation, it won‚Äôt be very long at all until we are talking with computers the way we talk with each other.
