---
layout: post
title: Could MR & AI make immersive product search easy?
subtitle: Matrix_Stockroom
video: https://cdn.shopify.com/videos/c/o/v/4058a0d0fe674f2d9ea3452865bc3885.mp4
---

We have recently been exploring what online shopping will look like on a spatial internet.

Think about how you buy things online these days. You type what you want into a search bar, and then you look at dozens if not hundreds of options before choosing one.

Now the question is: what would that look like in VR?

After talking about this problem for a while, one of us asked: “Do you guys remember that scene in The Matrix where Neo and Trinity are sent into an infinite stockroom?”

![_config.yml]({{ site.baseurl }}/images/matrix_stockroom/matrix_stockroom.jpg){:.centered}

We all sure did. That’s the kind of scene you never forget.

Presenting an infinite array of objects to a user as a response to a search query is interesting, but how will they navigate and reduce that array to find something they like? That’s where voice commands and large language models come in. Meta already offers a fantastic [Voice SDK](https://developer.oculus.com/documentation/unity/voice-sdk-overview/), so this is even possible today!

We see this combination of spatial search results and voice commands as one possible way of matching the internet’s infinite scale and powerful filters.

The video below illustrates this concept and serves as our homage to the Matrix ❤️.

{% include youtube.html id='DNzfp-M5jhQ?modestbranding=1&amp;showinfo=0&amp;rel=0' %}

One cool thing to note: the hands you see in the video above were motion-captured using a Meta Quest headset and a tool we open sourced called [handy]({{ site.baseurl }}{% link _posts/2023-01-16-Handy.md %}).

Animating hands by hand (no pun intended) is horrendously complicated, so we hope this tool helps you if you ever want to add hands to a concept video or cutscene.
